# -*- coding: utf-8 -*-
"""PPO_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gakwiKHcwPx1LRSi-rRgmk5bBHI5bAbw
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2

# Commented out IPython magic to ensure Python compatibility.
# %pip install transformers trl wandb

import torch
from tqdm import tqdm
import pandas as pd

tqdm.pandas()

from transformers import pipeline, AutoTokenizer
from datasets import load_dataset

from trl import PPOTrainer, PPOConfig, AutoModelForCausalLMWithValueHead
from trl.core import LengthSampler

reward_df=pd.read_csv("reward_data.csv")
df_train=reward_df[:90]
df_test=reward_df[90:]
df_test=df_test.reset_index()
df1=pd.DataFrame({"text":["<s>[INST] Summarize the question based on the context below. " + "\n[context]: " + df_train['article'][i] + "[\\INST] "+df_train['summary'][i] for i in range(len(df_train))]})
df2=pd.DataFrame({"text":["<s>[INST] Summarize the question based on the context below. " + "\n[context]: " + df_test['article'][i] + "[\\INST] "+df_test['summary'][i] for i in range(len(df_test))]})
df=pd.DataFrame({"text":["<s>[INST] Summarize the question based on the context below. " + "\n[context]: " + reward_df['article'][i] + "[\\INST] "+reward_df['summary'][i] for i in range(len(reward_df))]})
df1['label']=df_train['output']
df2['label']=df_test['output']
df['label']=reward_df['output']
df1['summary']=[df_train['summary'][i] for i in range(len(df_train))]
df2['summary']=[df_test['summary'][i] for i in range(len(df_test))]
df['summary']=[reward_df['summary'][i] for i in range(len(reward_df))]

df_train_for_gen=pd.DataFrame({"text":["<s>[INST] Summarize the question based on the context below. " + "\n[context]: " + df_train['article'][i] + "[\INST] " for i in range(len(df_train)) if len(df_train['article'][i].split(" "))<700]})
df_train_for_gen['summary']=df1['summary']
df_test_for_gen=pd.DataFrame({"text":["<s>[INST] Summarize the question based on the context below. " + "\n[context]: " + df_test['article'][i] + "[\INST] " for i in range(len(df_test)) if len(df_test['article'][i].split(" "))<700]})
df_test_for_gen['summary']=df2['summary']
df_for_gen=pd.DataFrame({"text":["<s>[INST] Summarize the question based on the context below. " + "\n[context]: " + reward_df['article'][i] + "[\INST] " for i in range(len(reward_df)) if len(reward_df['article'][i].split(" "))<700]})
df_for_gen['summary']=df['summary']

from datasets import *
ds3=Dataset.from_pandas(df_for_gen)
ds1=ds3.train_test_split(test_size=0.1)
ds2=DatasetDict({'train':ds1['train'],
                 'test':ds1['test']})
ds2

#from datasets import load_dataset

#dataset = load_dataset("HuggingFaceH4/cherry_picked_prompts", split="train")
ds1 = ds2.rename_column("text", "query")
#dataset = dataset.remove_columns(["meta", "completion"])

from trl import PPOConfig

config = PPOConfig(
    model_name="Ritirupa/sft-llama-2-7b-sft",
    learning_rate=1.41e-5,
)

from transformers import AutoTokenizer

from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer

model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)
tokenizer = AutoTokenizer.from_pretrained(config.model_name)

tokenizer.pad_token = tokenizer.eos_token

from transformers import pipeline

reward_model = pipeline("text-classification", model="Ritirupa/gpt2_reward_test",tokenizer="Ritirupa/gpt2_reward_test")

def tokenize(sample):
    sample["input_ids"] = tokenizer.encode(sample["query"])
    return sample

dataset = ds1.map(tokenize, batched=False)

from trl import PPOTrainer

ppo_trainer = PPOTrainer(
    model=model,
    config=config,
    dataset=dataset,
    tokenizer=tokenizer,
)

generation_kwargs = {
    "min_length": -1,
    "top_k": 0.0,
    "top_p": 1.0,
    "do_sample": True,
    "pad_token_id": tokenizer.eos_token_id,
}

from tqdm import tqdm

for epoch, batch in tqdm(enumerate(ppo_trainer.dataloader)):
    query_tensors = batch["input_ids"]

    #### Get response from SFTModel
    response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)
    batch["response"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]

    #### Compute reward score
    texts = [q + r for q, r in zip(batch["query"], batch["response"])]
    pipe_outputs = reward_model(texts)
    rewards = [torch.tensor(output[1]["score"]) for output in pipe_outputs]

    #### Run PPO step
    stats = ppo_trainer.step(query_tensors, response_tensors, rewards)
    ppo_trainer.log_stats(stats, batch, rewards)

#### Save model
ppo_trainer.save_model("my_ppo_model")